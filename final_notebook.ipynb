{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize, sent_tokenize, TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                               text  Target\n",
      "0        1  @USAirways  ! THE WORST in customer service. @...      -1\n",
      "1        2  @united call wait times are over 20 minutes an...      -1\n",
      "2        3  @JetBlue what's up with the random delay on fl...      -1\n",
      "3        4  @AmericanAir Good morning!  Wondering why my p...       0\n",
      "4        5  @united UA 746. Pacific Rim and Date Night cut...      -1\n",
      "...    ...                                                ...     ...\n",
      "7315  7316                            @AmericanAir followback       0\n",
      "7316  7317  @united thanks for the help. Wish the phone re...       1\n",
      "7317  7318  @usairways the. Worst. Ever. #dca #customerser...      -1\n",
      "7318  7319  @nrhodes85: look! Another apology. DO NOT FLY ...      -1\n",
      "7319  7320  @united you are by far the worst airline. 4 pl...      -1\n",
      "\n",
      "[7320 rows x 3 columns]\n",
      "         id                                               text\n",
      "0      7322  @AmericanAir In car gng to DFW. Pulled over 1h...\n",
      "1      7323  @AmericanAir after all, the plane didnÂÃÂªt ...\n",
      "2      7324  @SouthwestAir can't believe how many paying cu...\n",
      "3      7325  @USAirways I can legitimately say that I would...\n",
      "4      7326  @AmericanAir still no response from AA. great ...\n",
      "...     ...                                                ...\n",
      "7315  14637  @JetBlue Traveling with two kids tomorrow (age...\n",
      "7316  14638  @JetBlue Tx for the info. Just don't understan...\n",
      "7317  14639  @AmericanAir I understand. But why is this the...\n",
      "7318  14640                               @USAirways really!??\n",
      "7319  14641  @united no I did not make connection.  Your st...\n",
      "\n",
      "[7320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#file import\n",
    "#===========\n",
    "\n",
    "df_train = pd.read_csv(\"../input/nlphw1/train.csv\", encoding = \"ISO-8859-1\")\n",
    "print(df_train) #.describe()\n",
    "\n",
    "df_test = pd.read_csv(\"../input/nlphw1/test.csv\", encoding = \"ISO-8859-1\")\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                               text  Target\n",
      "0        1  @usairways   the worst in customer service @us...      -1\n",
      "1        2  @united call wait times are over  minutes and ...      -1\n",
      "2        3  @jetblue whats up with the random delay on fli...      -1\n",
      "3        4  @americanair good morning  wondering why my pr...       0\n",
      "4        5  @united ua  pacific rim and date night cut out...      -1\n",
      "...    ...                                                ...     ...\n",
      "7315  7316                            @americanair followback       0\n",
      "7316  7317  @united thanks for the help wish the phone rep...       1\n",
      "7317  7318      @usairways the worst ever dca customerservice      -1\n",
      "7318  7319  @nrhodes look another apology do not fly @usai...      -1\n",
      "7319  7320  @united you are by far the worst airline  plan...      -1\n",
      "\n",
      "[7320 rows x 3 columns]\n",
      "         id                                               text\n",
      "0      7322  @americanair in car gng to dfw pulled over hr ...\n",
      "1      7323  @americanair after all the plane didnâãâªt lan...\n",
      "2      7324  @southwestair cant believe how many paying cus...\n",
      "3      7325  @usairways i can legitimately say that i would...\n",
      "4      7326  @americanair still no response from aa great j...\n",
      "...     ...                                                ...\n",
      "7315  14637  @jetblue traveling with two kids tomorrow ages...\n",
      "7316  14638  @jetblue tx for the info just dont understand ...\n",
      "7317  14639  @americanair i understand but why is this the ...\n",
      "7318  14640                                  @usairways really\n",
      "7319  14641  @united no i did not make connection  your ste...\n",
      "\n",
      "[7320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#pre-processing\n",
    "#=================\n",
    "\n",
    "df_train[\"text\"]=df_train[\"text\"].str.lower()  #convert to lower case\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace('\\d+','') # remove numbers\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace('<.*?>','') # remove HTML tags\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace('[^@\\w\\s]','') # remove punctuation  \n",
    "\n",
    "df_test[\"text\"]=df_test[\"text\"].str.lower()  #convert to lower case\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace('\\d+','') # remove numbers\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace('<.*?>','') # remove HTML tags\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace('[^@\\w\\s]','') # remove punctuation  \n",
    "\n",
    "print(df_train)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                               text  Target\n",
      "0        1  @usairway the worst in custom servic @usairway...      -1\n",
      "1        2  @unit call wait time are over minut and airpor...      -1\n",
      "2        3  @jetblu what up with the random delay on fligh...      -1\n",
      "3        4  @americanair good morn wonder whi my pretsa ch...       0\n",
      "4        5  @unit ua pacif rim and date night cut out not ...      -1\n",
      "...    ...                                                ...     ...\n",
      "7315  7316                            @americanair followback       0\n",
      "7316  7317  @unit thank for the help wish the phone rep co...       1\n",
      "7317  7318        @usairway the worst ever dca customerservic      -1\n",
      "7318  7319     @nrhode look anoth apolog do not fli @usairway      -1\n",
      "7319  7320  @unit you are by far the worst airlin plane de...      -1\n",
      "\n",
      "[7320 rows x 3 columns]\n",
      "         id                                               text\n",
      "0      7322  @americanair in car gng to dfw pull over hr ag...\n",
      "1      7323  @americanair after all the plane didnâãâªt lan...\n",
      "2      7324  @southwestair cant believ how mani pay custom ...\n",
      "3      7325  @usairway i can legitim say that i would have ...\n",
      "4      7326  @americanair still no respons from aa great jo...\n",
      "...     ...                                                ...\n",
      "7315  14637  @jetblu travel with two kid tomorrow age and d...\n",
      "7316  14638  @jetblu tx for the info just dont understand w...\n",
      "7317  14639  @americanair i understand but whi is this the ...\n",
      "7318  14640                                   @usairway realli\n",
      "7319  14641  @unit no i did not make connect your stellar e...\n",
      "\n",
      "[7320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "#===========\n",
    "\n",
    "st = SnowballStemmer('english')\n",
    "df_train['text'] =df_train['text'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "print(df_train)\n",
    "\n",
    "\n",
    "df_test['text'] =df_test['text'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "print(df_test)\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#df['Question']=df_train['text'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                               text  Target\n",
      "0        1  @usairway worst custom servic @usairway call m...      -1\n",
      "1        2  @unit call wait time minut airport wait time l...      -1\n",
      "2        3   @jetblu random delay flight ani chanc fals alarm      -1\n",
      "3        4  @americanair good morn wonder whi pretsa check...       0\n",
      "4        5  @unit ua pacif rim date night cut constant ran...      -1\n",
      "...    ...                                                ...     ...\n",
      "7315  7316                            @americanair followback       0\n",
      "7316  7317      @unit thank help wish phone rep could accomid       1\n",
      "7317  7318            @usairway worst ever dca customerservic      -1\n",
      "7318  7319            @nrhode look anoth apolog fli @usairway      -1\n",
      "7319  7320  @unit far worst airlin plane delay round trip ...      -1\n",
      "\n",
      "[7320 rows x 3 columns]\n",
      "         id                                               text\n",
      "0      7322  @americanair car gng dfw pull hr ago veri ici ...\n",
      "1      7323  @americanair plane didnâãâªt land ident wors c...\n",
      "2      7324  @southwestair cant believ mani pay custom left...\n",
      "3      7325  @usairway legitim say would rather driven cros...\n",
      "4      7326        @americanair still respons aa great job guy\n",
      "...     ...                                                ...\n",
      "7315  14637  @jetblu travel two kid tomorrow age domest nee...\n",
      "7316  14638  @jetblu tx info dont understand whi couldnt ac...\n",
      "7317  14639  @americanair understand whi onli flight day go...\n",
      "7318  14640                                   @usairway realli\n",
      "7319  14641  @unit make connect stellar employe vicki thoma...\n",
      "\n",
      "[7320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "#====================\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "df_train['text'] = df_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "print(df_train)\n",
    "\n",
    "df_test['text'] = df_test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                               text  Target\n",
      "0        1  worst custom servic call month book flight poo...      -1\n",
      "1        2      call wait time minut airport wait time longer      -1\n",
      "2        3           random delay flight ani chanc fals alarm      -1\n",
      "3        4  good morn wonder whi pretsa check board pass morn       0\n",
      "4        5  ua pacif rim date night cut constant random on...      -1\n",
      "...    ...                                                ...     ...\n",
      "7315  7316                                         followback       0\n",
      "7316  7317            thank help wish phone rep could accomid       1\n",
      "7317  7318                      worst ever dca customerservic      -1\n",
      "7318  7319                              look anoth apolog fli      -1\n",
      "7319  7320  far worst airlin plane delay round trip flight...      -1\n",
      "\n",
      "[7320 rows x 3 columns]\n",
      "         id                                               text\n",
      "0      7322  car gng dfw pull hr ago veri ici road onhold a...\n",
      "1      7323  plane didnâãâªt land ident wors condit grk acc...\n",
      "2      7324  cant believ mani pay custom left high dri reas...\n",
      "3      7325  legitim say would rather driven cross countri ...\n",
      "4      7326                     still respons aa great job guy\n",
      "...     ...                                                ...\n",
      "7315  14637  travel two kid tomorrow age domest need birth ...\n",
      "7316  14638  tx info dont understand whi couldnt accur esti...\n",
      "7317  14639  understand whi onli flight day go twice im ext...\n",
      "7318  14640                                             realli\n",
      "7319  14641  make connect stellar employe vicki thoma refus...\n",
      "\n",
      "[7320 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#word tokenizer\n",
    "#===============\n",
    "# using tweetokenizer to remove @ handles. WordPunctTokenizer does not keep handles together.\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "df_train['text']=df_train['text'].apply(lambda row: \" \".join(tknzr.tokenize(row)))\n",
    "print(df_train)\n",
    "\n",
    "df_test['text']=df_test['text'].apply(lambda row: \" \".join(tknzr.tokenize(row)))\n",
    "print(df_test)\n",
    "\n",
    "#sentence tokenizer\n",
    "#===================\n",
    "\n",
    "#sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#df.apply(lambda row: nltk.word_tokenize(row['frases']), axis=1)\n",
    "#df_train['text'].apply(lambda row: sent_segmenter.tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_TF - Best cross-validation score: 0.68\n",
      "LR_TF - Best cross-validation score: {'logisticregression__C': 0.1, 'tfidfvectorizer__ngram_range': (1, 1)}\n",
      "\n",
      "LR_HV - Best cross-validation score: 0.70\n",
      "LR_HV - Best cross-validation score: {'HashingVectorizer__ngram_range': (1, 1), 'logisticregression__C': 0.1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "#========================\n",
    "pipe_LR_TF = Pipeline([('tfidfvectorizer', TfidfVectorizer()), ('logisticregression',LogisticRegression(solver='lbfgs'))]) \n",
    "param_grid_LR_TF = {'logisticregression__C': [0.001, 0.01, 0.1], \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2)]}\n",
    "grid_LR_TF = GridSearchCV(pipe_LR_TF, param_grid_LR_TF, cv=5)\n",
    "grid_LR_TF.fit(df_train['text'], df_train['Target'])\n",
    "print(\"LR_TF - Best cross-validation score: {:.2f}\".format(grid_LR_TF.best_score_))\n",
    "print(\"LR_TF - Best cross-validation score: {0}\\n\".format(grid_LR_TF.best_params_))\n",
    "\n",
    "pipe_LR_HV =Pipeline([('HashingVectorizer', HashingVectorizer()), ('logisticregression',LogisticRegression(solver='lbfgs'))])  \n",
    "#make_pipeline(HashingVectorizer(stop_words='english'), LogisticRegression(solver='lbfgs'))\n",
    "param_grid_LR_HV = {'logisticregression__C': [0.001, 0.01, 0.1], \"HashingVectorizer__ngram_range\": [(1, 1), (1, 2)]}\n",
    "grid_LR_HV = GridSearchCV(pipe_LR_HV, param_grid_LR_HV, cv=5)\n",
    "grid_LR_HV.fit(df_train['text'], df_train['Target'])\n",
    "print(\"LR_HV - Best cross-validation score: {:.2f}\".format(grid_LR_HV.best_score_))\n",
    "print(\"LR_HV - Best cross-validation score: {0}\\n\".format(grid_LR_HV.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes requires todense() option\n",
    "#===========================================\n",
    "#pipe_GS_TF = Pipeline([('tfidfvectorizer', TfidfVectorizer()), ('GaussianNB', GaussianNB())]) \n",
    "#param_grid_GS_TF = {\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2)]}\n",
    "#grid_GS_TF = GridSearchCV(pipe_GS_TF, param_grid_GS_TF, cv=5)\n",
    "#grid_GS_TF.fit(df_train['text'], df_train['Target'])\n",
    "#print(\"GS_TF - Best cross-validation score: {:.2f}\".format(grid_GS_TF.best_score_))\n",
    "#print(\"GS_TF - Best cross-validation score: {0}\\n\".format(grid_GS_TF.best_params_))\n",
    "\n",
    "#pipe_GS_HV =Pipeline([('HashingVectorizer', HashingVectorizer()), ('GaussianNB', GaussianNB())])  \n",
    "#make_pipeline(HashingVectorizer(stop_words='english'), LogisticRegression(solver='lbfgs'))\n",
    "#param_grid_GS_HV = {\"HashingVectorizer__ngram_range\": [(1, 1), (1, 2)]}\n",
    "#grid_GS_HV = GridSearchCV(pipe_GS_HV, param_grid_GS_HV, cv=5)\n",
    "#grid_GS_HV.fit(df_train['text'], df_train['Target'])\n",
    "#print(\"GS_HV - Best cross-validation score: {:.2f}\".format(grid_GS_HV.best_score_))\n",
    "#print(\"GS_HV - Best cross-validation score: {0}\\n\".format(grid_GS_HV.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG_TF - Best cross-validation score: 0.70\n",
      "XG_TF - Best cross-validation score: {'XGBClassifier__learning_rate': 0.01, 'XGBClassifier__max_depth': 10, 'XGBClassifier__n_estimators': 25, 'tfidfvectorizer__ngram_range': (1, 1)}\n",
      "\n",
      "XG_HV - Best cross-validation score: 0.67\n",
      "XG_HV - Best cross-validation score: {'HashingVectorizer__ngram_range': (1, 1), 'XGBClassifier__learning_rate': 0.01, 'XGBClassifier__max_depth': 7, 'XGBClassifier__n_estimators': 50}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "#==========\n",
    "pipe_XG_TF = Pipeline([('tfidfvectorizer', TfidfVectorizer()), ('XGBClassifier',XGBClassifier())]) \n",
    "param_grid_XG_TF = {\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2)], 'XGBClassifier__n_estimators': [25,50],'XGBClassifier__max_depth': [7,10], 'XGBClassifier__learning_rate': [0.01,0.001]}\n",
    "grid_XG_TF = GridSearchCV(pipe_XG_TF, param_grid_XG_TF, cv=5)\n",
    "grid_XG_TF.fit(df_train['text'], df_train['Target'])\n",
    "print(\"XG_TF - Best cross-validation score: {:.2f}\".format(grid_XG_TF.best_score_))\n",
    "print(\"XG_TF - Best cross-validation score: {0}\\n\".format(grid_XG_TF.best_params_))\n",
    "\n",
    "pipe_XG_HV =Pipeline([('HashingVectorizer', HashingVectorizer(n_features=20)), ('XGBClassifier',XGBClassifier())])  \n",
    "#make_pipeline(HashingVectorizer(stop_words='english'), LogisticRegression(solver='lbfgs'))\n",
    "param_grid_XG_HV = {\"HashingVectorizer__ngram_range\": [(1, 1), (1, 2)],'XGBClassifier__n_estimators': [25,50],'XGBClassifier__max_depth': [7,10], 'XGBClassifier__learning_rate': [0.01,0.001]}\n",
    "grid_XG_HV = GridSearchCV(pipe_XG_HV, param_grid_XG_HV, cv=5)\n",
    "grid_XG_HV.fit(df_train['text'], df_train['Target'])\n",
    "print(\"XG_HV - Best cross-validation score: {:.2f}\".format(grid_XG_HV.best_score_))\n",
    "print(\"XG_HV - Best cross-validation score: {0}\\n\".format(grid_XG_HV.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:40:38] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { ngram_range } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with Hash vectorization, and XG Boost with Tfidf give cross validation accuracy of 70%. \n",
    "# Logistic regression with TF-IDF has 68% accuracy. \n",
    "#Evaluating predicted test results\n",
    "\n",
    "\n",
    "#Logistic regression with Hash vectorization\n",
    "#===========================================\n",
    "hashvector = HashingVectorizer(ngram_range=(1,1))\n",
    "train_vectors_LR_HV = hashvector.fit_transform(df_train['text']) \n",
    "\n",
    "test_vectors = hashvector.transform(df_test['text'])\n",
    "\n",
    "model_LR_HV=LogisticRegression(solver='lbfgs', C=0.1)\n",
    "\n",
    "model_LR_HV.fit(train_vectors_LR_HV,df_train['Target'])\n",
    "y_pred_LR_HV = model_LR_HV.predict(test_vectors)\n",
    "\n",
    "df_test['Target_LR_HV']=y_pred_LR_HV\n",
    "\n",
    "#print(confusion_matrix(df_train['Target'], y_pred_LR_HV))\n",
    "\n",
    "\n",
    "#XG Boost with Tfidf vectorization\n",
    "#===================================\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    "train_vectors_XG_TF = tfidf.fit_transform(df_train['text'])  \n",
    "\n",
    "test_vectors_XG_TF = tfidf.transform(df_test['text'])\n",
    "\n",
    "model_XG_TF=XGBClassifier(learning_rate=0.01,max_depth=10,n_estimators=25,ngram_range=(1, 1))\n",
    "\n",
    "model_XG_TF.fit(train_vectors_XG_TF,df_train['Target'])\n",
    "\n",
    "y_pred_XG_TF = model_XG_TF.predict(test_vectors_XG_TF)\n",
    "\n",
    "df_test['Target']=y_pred_XG_TF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the comparison, we see that the sentence (after stemming) with id = 7326 is classified as negative sentiment in 1st model \n",
    "# and classified as positive sentiment in 2nd model. So 1st model does a better job.\n",
    "\n",
    "header = [\"id\", \"Target\"]\n",
    "df_test.to_csv('submission.csv', columns = header, index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
